{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a parser for the program, specify column names for the data and assign a default value for each column if the data is not available in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--test_data'], dest='test_data', nargs=None, const=None, default='../../data/valid_phase5.csv', type=<class 'str'>, choices=None, help='Path to the test data.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase = 5\n",
    "\n",
    "column_default = [\n",
    "    ('', 0), \n",
    "    ('msno','missing_msno'),\n",
    "    ('song_id','missing_song_id'),\n",
    "    ('source_system_tab','missing_tab'),\n",
    "    ('source_screen_name','missing_screen'),\n",
    "    ('source_type','missing_source_type'),\n",
    "    ('target',''),\n",
    "    ('song_length',241812.0),\n",
    "    ('composer','empty'),\n",
    "    ('lyricist','empty'),\n",
    "    ('language',0.0),\n",
    "    ('short_song',0.0),\n",
    "    ('mean_length_distance',0.0),\n",
    "    ('city',-1.0),\n",
    "    ('bd',29.0),\n",
    "    ('gender','empty'),\n",
    "    ('registered_via',0),\n",
    "    ('membership_days', 0),\n",
    "    ('registration_timestamp',0.0),\n",
    "    ('expiration_timestamp',0.0),\n",
    "    ('country_code','empty'),\n",
    "    ('issuer','empty'),\n",
    "    ('issue_year',0.0),\n",
    "    ('unique_id','empty'),\n",
    "    ('genre_ids_count',0),\n",
    "    ('lyricists_count',0),\n",
    "    ('composer_count',0),\n",
    "    ('artist_count',0),\n",
    "    ('is_featured',0),\n",
    "    ('feat_artist','empty'),\n",
    "    ('main_artist','empty'),\n",
    "    ('feat_song_name','empty'),\n",
    "    ('main_song_name','empty'),\n",
    "    ('is_remix',0),\n",
    "    ('is_live',0),\n",
    "    ('is_acoustic',0),\n",
    "    ('is_instrumental',0),\n",
    "    ('artist_is_composer',0),\n",
    "    ('artist_is_composer_is_lyricist',0),\n",
    "    ('song_lang_magic',0),\n",
    "    ('count_song_played',425),\n",
    "    ('count_artist_played',406),\n",
    "    ('count_composer_played',171),\n",
    "    ('count_lyricist_played',226),\n",
    "    ('count_member_action',848),\n",
    "    ('member_action_per_day',0.0),\n",
    "    ('artist_main','empty'),\n",
    "    ('song_main','empty'),\n",
    "    ('artist_in_parenthesis','empty'),\n",
    "    ('artist_in_titlemark','empty'),\n",
    "    ('song_pre_parenthesis','empty'),\n",
    "    ('song_in_parenthesis','empty'),\n",
    "    ('song_pre_titlemark','empty'),\n",
    "    ('song_in_titlemark','empty'),\n",
    "    ('genre_pca_1',0.0),\n",
    "    ('genre_pca_2',0.0),\n",
    "    ('genre_pca_3',0.0),\n",
    "    ('genre_pca_4',0.0),\n",
    "    ('genre_pca_5',0.0),\n",
    "    ('genre_pca_6',0.0),\n",
    "    ('genre_pca_7',0.0),\n",
    "    ('genre_pca_8',0.0),\n",
    "    ('genre_pca_9',0.0),\n",
    "    ('genre_pca_10',0.0),\n",
    "    ('genre_pca_11',0.0),\n",
    "    ('genre_pca_12',0.0),\n",
    "    ('keyword_cluster_pca_0', 0.0),\n",
    "    ('keyword_cluster_pca_1', 0.0),\n",
    "]\n",
    "\n",
    "_CSV_COLUMNS = []\n",
    "_CSV_COLUMN_DEFAULTS = []\n",
    "\n",
    "# print(column_default)\n",
    "\n",
    "for key in column_default:\n",
    "    _CSV_COLUMNS.append(key[0])\n",
    "    _CSV_COLUMN_DEFAULTS.append([key[1]])\n",
    "\n",
    "#print(_CSV_COLUMNS)\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model_dir', type=str, default='/tmp/census_model',\n",
    "    help='Base directory for the model.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model_type', type=str, default='wide_deep',\n",
    "    help=\"Valid model types: {'wide', 'deep', 'wide_deep'}.\")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--train_epochs', type=int, default=40, help='Number of training epochs.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--epochs_per_eval', type=int, default=2,\n",
    "    help='The number of training epochs to run between evaluations.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_size', type=int, default=256, help='Number of examples per batch.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--train_data', type=str, default='../../data/train_phase' + str(phase) + '.csv',\n",
    "    help='Path to the training data.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--test_data', type=str, default='../../data/valid_phase' + str(phase) + '.csv',\n",
    "    help='Path to the test data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_NUM_EXAMPLES = {\n",
    "    'train': 5901935 + 1474484 + 1000,# - 300000,\n",
    "    'validation': 0#300000,#1474484 + 1000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give parameters for hashing categorical features. Create a crossed feature with several categorical features. Feed these features into the wide neural network. Embed several categorical features to put them and other numerical features in the deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_columns():\n",
    "  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
    "  # Continuous columns\n",
    "  categorical_features = {}\n",
    "  numerical_features = {}\n",
    "  for key in column_default:\n",
    "    if type(key[1])!=str:\n",
    "        numerical_features[key[0]] = tf.feature_column.numeric_column(key[0])\n",
    "    elif key[0] != 'target':\n",
    "        categorical_features[key[0]] = tf.feature_column.categorical_column_with_hash_bucket(key[0], hash_bucket_size=3200000)\n",
    "\n",
    "  categorical_features['gender'] = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'gender', ['male', 'female'])\n",
    "        \n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.feature_column.bucketized_column(\n",
    "      numerical_features['bd'], boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "  song_length_buckets = tf.feature_column.bucketized_column(\n",
    "      numerical_features['song_length'], boundaries=[160000, 320000, 480000, 640000])\n",
    "\n",
    "  # Wide columns and deep columns.\n",
    "  base_columns = []\n",
    "  for key in categorical_features:\n",
    "    base_columns.append(categorical_features[key])\n",
    "\n",
    "\n",
    "  wide_columns = base_columns #+ crossed_columns\n",
    "\n",
    "  deep_columns = []\n",
    "  for key in numerical_features:\n",
    "    deep_columns.append(numerical_features[key])\n",
    "  for key in categorical_features:\n",
    "    if (key=='song_length'):\n",
    "      deep_columns.append(tf.feature_column.numeric_column('song_length', normalizer_fn=lambda x: (x - 241812.0) / 67351))\n",
    "    else:\n",
    "      deep_columns.append(tf.feature_column.embedding_column(categorical_features[key], 24))\n",
    "\n",
    "  return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify number of hidden layers and hidden units in the deep neural network. Return a wide network estimator, a deep network estimator or a wide and deep network estimator according to the model type provided in the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "  wide_columns, deep_columns = build_model_columns()\n",
    "  hidden_units = [128, 128, 64, 64, 32, 32]#[100, 75, 50, 25]\n",
    "\n",
    "  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
    "  # trains faster than GPU for this model.\n",
    "  run_config = tf.estimator.RunConfig().replace(\n",
    "      keep_checkpoint_max=2, session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "  #config.intra_op_parallelism_threads = 44\n",
    "  #config.intra_op_parallelism_threads = 44\n",
    "    \n",
    "  if model_type == 'wide':\n",
    "    return tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=wide_columns,\n",
    "        config=run_config)\n",
    "  elif model_type == 'deep':\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=hidden_units,\n",
    "        dropout=0.05,\n",
    "        config=run_config)\n",
    "  else:\n",
    "    return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        dnn_dropout=0.05,\n",
    "        config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a function that produces input for the tensorflow graph. The function will parse the csv file, do the shuffling, extract the label, and feed the features to the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "\n",
    "  def parse_csv(value):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(value, field_delim = ',', record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('target')\n",
    "    return features, tf.equal(labels, '1')\n",
    "\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "  dataset = dataset.skip(1)\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
    "\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=5) #skip(7376419)\n",
    "\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logger that helps to keep track of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog5.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the wide and deep neural network for epochs specified in the parser, and then predict the labels for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, epoch, phase):\n",
    "  fea_str = \",id,msno,song_id,source_system_tab,source_screen_name,source_type,song_length,composer,lyricist,language,short_song,mean_length_distance,city,bd,gender,registered_via,membership_days,registration_timestamp,expiration_timestamp,country_code,issuer,issue_year,unique_id,genre_ids_count,lyricists_count,composer_count,artist_count,is_featured,feat_artist,main_artist,feat_song_name,main_song_name,is_remix,is_live,is_acoustic,is_instrumental,artist_is_composer,artist_is_composer_is_lyricist,song_lang_magic,count_song_played,count_artist_played,count_composer_played,count_lyricist_played,count_member_action,member_action_per_day,artist_main,song_main,artist_in_parenthesis,artist_in_titlemark,song_pre_parenthesis,song_in_parenthesis,song_pre_titlemark,song_in_titlemark,genre_pca_1,genre_pca_2,genre_pca_3,genre_pca_4,genre_pca_5,genre_pca_6,genre_pca_7,genre_pca_8,genre_pca_9,genre_pca_10,genre_pca_11,genre_pca_12,keyword_cluster_pca_0,keyword_cluster_pca_1\"\n",
    "  column_names2 = fea_str.split(\",\")\n",
    "\n",
    "  def conv(val):  \n",
    "    try:\n",
    "        return np.float(val)\n",
    "    except:        \n",
    "        return np.float64(0)\n",
    "\n",
    "  def conv1(val):  \n",
    "    try:\n",
    "        return np.float(val)\n",
    "    except:        \n",
    "        return np.float64(-1.0)\n",
    "\n",
    "  df = pd.read_csv('../../data/test_phase' + str(phase) + '.csv', index_col=False, names=column_names2,\n",
    "                            skip_blank_lines=True, keep_default_na=False, skiprows=1,\n",
    "                            converters={'language':conv1, 'mean_length_distance': conv1, \n",
    "                                        'issue_year':conv1, 'song_length': conv, 'short_song':conv1})\n",
    "  predictions = list(model.predict(input_fn=tf.estimator.inputs.pandas_input_fn(\n",
    "    x=df, num_epochs=1,shuffle=False)))\n",
    "  res = pd.DataFrame(predictions)\n",
    "  #temp = model\n",
    "  #temp2 = predictions\n",
    "  res.to_csv('./pred' + str(phase) + '_' + str(epoch + 1)+ '.csv', sep = ',', header = False)\n",
    "  column_names3=['id', 'label', 'score', 'target', 's2', 's']\n",
    "  def conv3(val):  \n",
    "    return np.int(val[1])\n",
    "  def conv4(val):\n",
    "    return np.float(val[1:-1])\n",
    "  census_pred = pd.read_csv('./pred' + str(phase) + '_' + str(epoch + 1)+ '.csv', index_col=False, names=column_names3,\n",
    "                          skip_blank_lines=True, keep_default_na=False, converters={'target': conv4})\n",
    "  header=['id', 'target']\n",
    "  census_pred.to_csv('output_float_phase' + str(phase) + '_epoch_' + str(epoch + 1) + '.csv', index=False, columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_task_id': 0, '_num_ps_replicas': 0, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_log_step_count_steps': 100, '_save_checkpoints_secs': 600, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd8224ebe80>, '_task_type': 'worker', '_num_worker_replicas': 1, '_save_checkpoints_steps': None, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      ", '_keep_checkpoint_max': 2, '_service': None, '_model_dir': '/tmp/census_model'}\n",
      "Parsing ../../data/train_phase5.csv\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "temp2 = 0\n",
    "model = None\n",
    "def main(unused_argv):\n",
    "  global temp\n",
    "  global temp2\n",
    "  global model\n",
    "  # Clean up the model directory if present\n",
    "  shutil.rmtree(FLAGS.model_dir, ignore_errors=True)\n",
    "  model = build_estimator(FLAGS.model_dir, FLAGS.model_type)\n",
    "\n",
    "  # Train and evaluate the model every `FLAGS.epochs_per_eval` epochs.\n",
    "  for n in range(20):#range(FLAGS.train_epochs // FLAGS.epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        FLAGS.train_data, FLAGS.epochs_per_eval, True, FLAGS.batch_size))\n",
    "\n",
    "    '''results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        FLAGS.test_data, 1, False, FLAGS.batch_size))\n",
    "    for key in sorted(results):\n",
    "        print('%s: %s' % (key, results[key]))'''\n",
    "    \n",
    "    if n == 0 or (n + 1) % 5 == 0:\n",
    "      # Display evaluation metrics\n",
    "      logger.info(('Results at epoch', (n + 1) * FLAGS.epochs_per_eval))\n",
    "      print('-' * 60)\n",
    "      predict(model, n, phase)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed + [temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the labels from the prediction result to generate submission file output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
